**Целью итоговой работы по курсу "LLM Driven Development"** стала разработка двух ИИ-ассистентов, отличающихся 
функционалом и используемыми большими языковыми моделями.

**Основные требования:**
    Ассистенты должны были разрабатываться на основе предварительно обученных русскоязычных языковых моделей, 
выбранных с учётом имеющихся технических ограничений. Все взаимодействия пользователя и ассистентов должны 
регистрироваться специальными инструментами мониторинга. Разработку моделей следовало вести в рамках 
принципов CI/CD, а развертывание осуществлять методами контейнеризации.

**Архитектура и исходные коды**    
    Итоговая работа представлена проектом в виде пакета llm_ops_project со следующей структурой:

    llm_ops_project/
                    |
                    |-- config/               # Конфигурация проекта
                    |   |-- config.py         # Параметры конфигурации
                    |   |-- constants.py      # Константы проекта               
                    |
                    |-- data/                 # Папка с данными для ассистентов
                    |   |-- source_data/      # Сырые необработанные данные
                    |   |   |-- prompts.py    # Примеры вопросов для векторной базы данных
                    |   |   |-- phrases.txt   # Примеры исходных фраз персонажей
                    |   |
                    |   `-- processed_data/      # Предварительно обработанные данные
                    |       |-- final_data.json  # Результат предварительной обработки                      |
                    |       |-- phrases.csv      # Результат предварительной обработки 
                    |
                    |-- dockers/
                    |   |-- Dockerfile           # Инструкция для построения Docker-образа                    
                    |   |-- docker-compose.yml   # Состав проекта для разворачивания с Docker Compose                    
                    |
                    |-- docs/                    # Документы и документация
                    |   |-- DockerDesktop.png    # Скрин-шот Docker Desktop
                    |   |-- project_schema.png   # Графическая схема архитектуры проекта
                    |   |-- Запуск docker-image-1.png   # Скрин-шот работы приложения
                    |   |-- Запуск docker-image-2.png   # Скрин-шот работы приложения
                    |
                    |-- tests/                # Каталог тестов и проверок
                    |   |-- server_tmbot.py   # Тестирование через Telegram-бот
                    |   |-- cli_ui.py         # Интерфейс командной строки для тестирования ассистентов
                    |
                    |-- utils/                # Утилиты и служебные модули
                    |   |-- clear_data.py     # Скрипт для очистки и подготовки данных
                    |   |-- check_model.py    # Скрипт проверки доступности модели
                    |   |-- data_schema.py    # Модуль сериализации данных с помощью Pydantic
                    |
                    |-- main.py              # Главный исполняемый файл проекта
                    |-- .env                 # Конфигурационные переменные (ключ Hugging Face)
                    |-- requirements.txt     # Окружение проекта 
                    |-- README.md            # Описание проекта
    
    Архитектура проекта представлена графически в виде блок-схемы, расположенной в папке ![](/docs/project_schema.png).

    Исходники проекта опубликованы на платформе GitHub.    

**Описание пакета**
    Главный исполняемый файл проекта — main.py расположен в корневом каталоге проекта. Данный файл организует 
взаимодействие с пользователями посредством меню выбора режимов работы. При запуске программы последовательно 
выводится предложение выбрать модули для выполнения в формате "вопрос-ответ": введением "0" соответствующий модуль 
игнорируется, введением "1" — активируется ![](docs\Запуск docker-image-1.png). Реализованные модули включают:

    - предобработку данных для AI-собеседника (./utils/clear_data);
    - сериализацию данных с применением библиотеки Pydantic для AI-финансого консультанта (./utils/data_schema);
    - проверку доступности и производительности модели RugPT3Small (./utils/check_model.py);    
    - тестирование ассистентов локально через CLI-интерфейс (./tests/cli_ui.py);
    - тестирование ассистентов через Telegram-бот (./tests/server_tmbot.py);

   Запуск сервера для взаимодействия с пользователями через Telegram-бот и формирование ответов модели 
осуществляется последним этапом. После завершения процесса отправки сообщений через Telegram-бот (@MyTODO1746_Bot) 
становится возможным. По завершении сессии необходимо остановить работу сервера комбинацией клавиш CTRL + C.

    Демонстрационная версия работы приложения представлена двумя способами ввода запросов пользователя:
        
        - автономный режим (/tests/cli_ui.py): ввод команд осуществляется непосредственно через терминал 
          посредством строки пользовательского ввода;

        - режим с использованием телекоммуникационных сетей («Интернет») (/tests/server_tmbot_ui.py): 
          запросы отправляются через Telegram-бота на удалённый сервер для последующей обработки.
          (https://t.me/MyTODO1746_Bot)

**Данные**
    Данные для ассистентов расположены в папке ./data и подразделяются на два типа:

    - Исходные данные: находятся в папке ./data/source_data и загружаются соответствующими модулями при исполнении.
    - Предобработанные данные: формируются в ходе работы модулей и располагаются в папке ./data/processed_data. 

    Готовые обработанные данные можно увидеть в файлах phrases.csv и final_data.json, а шаблоны запросов для работы 
с векторной базой данных FAISS приведены в словаре my_qa_prompts файла prompts.py.

    Для полноценного функционирования приложения необходим действительный ключ API для Hugging Face, указанный в файле .env.

**Мониторинг**
    Мониторинг за поведением моделей и проведением эксперимента доступен в двух режимах: с применением mlflow 
и в терминале через CLI-интерфейс.  Для корректной работы приложения необходимо запустить сервер командой: 
    
    mlflow server -p 5000

**Развертывание и эксплуатация**
     Приложение запускается через CLI-интерфейс следующим образом:

    - Запуск из исходных файлов осуществляется командой:

        python path_to_file/main.py

    - Запуск Docker-контейнера выполняется командой:

        docker run -it --rm llm-ops-img

    В проекте настроен автоматический запуск и развертывания с использованием GitHub Actions в рамках процессов CI/CD. 
В состав пакета входят два файла конфигурации workflows:

        - .github/workflows/s1_run_project.yaml — конфигурация для запуска проекта. (рабочий)
        - .github/workflows/s2_project_deployment.yaml — файл, ответственный за деплоймент готового решения (отладка)

    Для быстрого развёртывания планировалось собрать специальный Docker-образ <llm-ops-img>, который можно запустить простой командой:

        docker run -it --rm llm-ops-img
    
    Фрагмент запуска приложения из docker-контейнера приведен на скриншотах ![](docs\Запуск docker-image-1.png) и 
![](docs\Запуск docker-image-2.png), в соотвествующих файлах, расположенных в папке /docs

    Однако, к сожалению смонтировать более или менее рабочий образ не удалось. Результат, который получился составил более 20 GB 
и приведен на скриншоте в файле ![](/docs/DockerDesktop.png). Основную часть образа занимает torch, без него образ был 0,7 GB. 
Все попытки загрузить конкретную версию  torch для cpu сопровождались ошибками (некоторые из них приведены далее):

    - => [3/4] COPY . .                                                                                                 0.2s
        => ERROR [4/4] RUN pip install -r requirements.txt                                                                2.7s
        ------
        > [4/4] RUN pip install -r requirements.txt:
        2.249 Usage: pip [options]
        2.249
        2.250 ERROR: Invalid requirement: torch==2.3.1+cpu -f
        2.250 pip: error: -f option requires 1 argument
        2.250

     - ERROR: Could not find a version that satisfies the requirement torch==2.3.1+cpu (from versions: 1.13.0, 1.13.1, 2.0.0, 2.0.1, 
     2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0, 2.9.1) 
     7.100 ERROR: No matching distribution found for torch==2.3.1+cpu

     - 1.720 Collecting torch==2.3.1+cpu (from -r requirements.txt (line 12))
        2.223   ERROR: HTTP error 403 while getting 
        https://download.pytorch.org/whl/cpu/torch_stable.html/torch-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl
        2.223 ERROR: Could not install requirement torch==2.3.1+cpu from 
        https://download.pytorch.org/whl/cpu/torch_stable.html/torch-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl 
        (from -r requirements.txt (line 12)) because of HTTP error 403 Client Error: Forbidden for 
        url: https://download.pytorch.org/whl/cpu/torch_stable.html/torch-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl 
        for URL https://download.pytorch.org/whl/cpu/torch_stable.html/torch-2.3.1%2Bcpu-cp311-cp311-linux_x86_64.whl

    Корректность работы проекта была проверена перед размещением в репозитории GitHub.
  